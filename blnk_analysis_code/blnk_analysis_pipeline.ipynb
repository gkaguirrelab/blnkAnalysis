{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49c8fee",
   "metadata": {},
   "source": [
    "# BLNK Analysis Pipeline\n",
    "This notebook serves an interactive means to operate the BLNK extraction pipeline. Following the steps below, \n",
    "one may analyze videos of the eye and extract pupil and eyelids information therefrom, outputting the \n",
    "result as a MATLAB file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfac0e",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "To use the pipeline, one must first install [Pylids](https://github.com/piecesofmindlab/pylids), which is attached as a submodule to this GitHub repo. \n",
    "Follow their installation instructions first. This will generate a pylids conda environment you will use as the basis for this project into which\n",
    "we will install further libraries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f75ec2",
   "metadata": {},
   "source": [
    "### Ensuring Correct Kernel\n",
    "To ensure you are using the correct pylids kernel, we will first import some standard library functions to check \n",
    "the name of the kernel you are using and assert it is the correct one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55afc87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pathlib\n",
    "import importlib\n",
    "conda_kernel_name: str | None = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "assert conda_kernel_name == \"pylids\", f\"Conda environment: {conda_kernel_name} is not equal to pylids\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639a69b",
   "metadata": {},
   "source": [
    "### Installing Additional Libraries. \n",
    "After the pylids conda environment has been created, we have to also install \n",
    "additional libraries into that environment used by this project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pupil-detectors\n",
    "%pip install pye3d\n",
    "%pip install natsort \n",
    "%pip install hdf5storage\n",
    "%pip install scipy \n",
    "%pip install dill\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddede1",
   "metadata": {},
   "source": [
    "### Import Custom Libraries\n",
    "Next, we will import the custom libraries written to do the analysis for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94485105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc8...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the top level of htis repo \n",
    "blnk_analysis_dir: str = \"/Users/zacharykelly/Documents/MATLAB/projects/BLNK_pipeline\"\n",
    "assert os.path.exists(blnk_analysis_dir), f\"Path: {blnk_analysis_dir} does not exist\"\n",
    "sys.path.append(blnk_analysis_dir)\n",
    "\n",
    "from utility import video_io\n",
    "from blnk_analysis_code.utility import blnk_analysis_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb730537",
   "metadata": {},
   "source": [
    "## Step 2: Gather Paths to Videos for Analysis\n",
    "Now that our libraries are properly installed, we will not gather the path(s) to the videos that \n",
    "we would like to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f845d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will initialize the list of videos we want to analyze. \n",
    "# We will populate this list with either a single path or many paths later \n",
    "videos_to_analyze: list[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you simly want to analyze a single video, enter the path to the \n",
    "# video directory here \n",
    "single_video_path: str = \"/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/BLNK_raw/PuffLight/modulate/TEST_001/TEST_001_modulate_direction-Mel_contrast-0.40_phase-3.14_trial-001_dual.avi\"\n",
    "videos_to_analyze.append(single_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ccf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to analyze a directory of videos, simply enter the path to that directory here \n",
    "video_directory_path: str = \"/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/BLNK_raw/PuffLight/modulate/BLNK_1007\"\n",
    "videos_to_analyze.extend([os.path.join(video_directory_path, filename) \n",
    "                          for filename in os.listdir(video_directory_path) if filename.endswith(\".avi\")]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180fdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will assert that all paths are unique, exist and can be accessed (e.g. ensure they are local files if the directory is on DropBox)\n",
    "unique_videos_to_analyze: dict[str, None] = {video: None \n",
    "                                             for video in videos_to_analyze\n",
    "                                            } # We use a dictionary to preserve order as compared to set\n",
    "\n",
    "# After we have asserted that all of the videos are unique, let's ensure the paths exist and they can be accessed \n",
    "for path in unique_videos_to_analyze:\n",
    "    assert os.path.exists(path), f\"Path does not exist: {path}\"\n",
    "    try:\n",
    "        assert video_io.inspect_video_frame_count(path) != 0, f\"Frame count for path: {path} is 0\" \n",
    "    except:\n",
    "        raise Exception(f\"Cannot count frames for path: {path}. Check if the file is corrupted/online-only?\")\n",
    "    \n",
    "# Reassign videos to analyze to be the unique set \n",
    "videos_to_analyze = [video for video, _ in unique_videos_to_analyze.items()]\n",
    "assert len(videos_to_analyze) != 0, f\"No videos to analyze\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a39be",
   "metadata": {},
   "source": [
    "## Step 3: Defining Output Location\n",
    "Next, we will define where the output of the analysis of the desired videos should be placed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output folder \n",
    "output_folder_path: str = \"/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/BLNK_analysis/PuffLight/modulate/BLNK_1007\"\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "assert os.path.exists(output_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21172d44",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Videos \n",
    "Now, we will analyze the desired videos. Doing so involves an intermediate step of modifying the video \n",
    "video cropping and then padding to a certain size, while also blacking out overly white pixels. Before analyzing, we must define the values of these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4219c",
   "metadata": {},
   "source": [
    "### Parameter Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6549b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the crop box and target size \n",
    "t, b, l, r = 0, 240, 0, 325\n",
    "crop_box: tuple = (t, b, l, r)\n",
    "target_size: tuple = (480, 640)\n",
    "whiteness_threshold: int = 225\n",
    "\n",
    "# Generate a temp output directory for the intermediate stage videos\n",
    "temp_dir_path: str = './temp_blnk_pipeline'\n",
    "if(not os.path.exists(temp_dir_path)):\n",
    "    os.mkdir(temp_dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3550c",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dad5de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 1/34\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_adapt-05_L.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    9900\n",
      "  Duration of video [s]:  55.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9900/9900 [05:31<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp958xvy2n/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp958xvy2n/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp958xvy2n/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_adapt-05_L.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    9900\n",
      "  Duration of video [s]:  55.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9900/9900 [07:18<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5iv3g3fb/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5iv3g3fb/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5iv3g3fb/temp_BLNK_1007_modulate_adapt-05_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_adapt-05_R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    9900\n",
      "  Duration of video [s]:  55.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9900/9900 [05:29<00:00, 30.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpa7eejvlr/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpa7eejvlr/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpa7eejvlr/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_adapt-05_R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    9900\n",
      "  Duration of video [s]:  55.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9900/9900 [07:18<00:00, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpmy7_acuc/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpmy7_acuc/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpmy7_acuc/temp_BLNK_1007_modulate_adapt-05_RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/numpy/polynomial/polynomial.py:1362: RankWarning: The fit may be poorly conditioned\n",
      "  return pu._fit(polyvander, x, y, deg, rcond, full, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 2/34\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_direction-LMS_contrast-0.20_phase-0.00_trial-001_L.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    11160\n",
      "  Duration of video [s]:  62.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11160/11160 [06:11<00:00, 30.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpm9t6d7pt/temp_BLNK_1007_modulate_direction-LMS_contrast-0.20_phase-0.00_trial-001_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpm9t6d7pt/temp_BLNK_1007_modulate_direction-LMS_contrast-0.20_phase-0.00_trial-001_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpm9t6d7pt/temp_BLNK_1007_modulate_direction-LMS_contrast-0.20_phase-0.00_trial-001_LDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_BLNK_1007_modulate_direction-LMS_contrast-0.20_phase-0.00_trial-001_L.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    11160\n",
      "  Duration of video [s]:  62.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 9831/11160 [07:09<00:58, 22.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_num, video_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(videos_to_analyze):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(videos_to_analyze)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mblnk_analysis_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_dir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mthreshold_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhiteness_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/BLNK_pipeline/blnk_analysis_code/utility/blnk_analysis_pipeline.py:100\u001b[0m, in \u001b[0;36mpredict_eye_features\u001b[0;34m(filepath, output_folder_path, crop_box, target_size, temp_dir_path, threshold_value, visualize_results, overrwrite_existing)\u001b[0m\n\u001b[1;32m     97\u001b[0m video_io\u001b[38;5;241m.\u001b[39mframes_to_video(video_resized, temp_video_path, video_fps)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Extract the eye features for this video\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m eye_features, perimeter_info_dict \u001b[38;5;241m=\u001b[39m \u001b[43mextract_eye_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mvisualization_output_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualization_output_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mpupil_feature_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpylids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                                                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Repackage the features along with their metadata\u001b[39;00m\n\u001b[1;32m    110\u001b[0m eye_features_dict: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/BLNK_pipeline/blnk_analysis_code/utility/extract_eye_features.py:506\u001b[0m, in \u001b[0;36mextract_eye_features\u001b[0;34m(video, is_grayscale, visualize_results, visualization_output_filepath, safe_execution, pupil_feature_method, keypoint_threshold)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m actual_video_framecount \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(pupil_features), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo framecount (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_video_framecount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not equal to analyzed frames (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pupil_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Some frames may be corrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Then, extract the eyelid features \u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     eyelid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_eyelid_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# Do not visualize single features if we want all features  \u001b[39;49;00m\n\u001b[1;32m    508\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\t\t\t\t\t\t\t                              \u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mkeypoint_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeypoint_threshold\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m      \u001b[38;5;66;03m# Ensure the analysis was properly done (e.g. there were no silently corrupted frames not explictly caught with error by Pylids)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(safe_execution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/BLNK_pipeline/blnk_analysis_code/utility/extract_eye_features.py:454\u001b[0m, in \u001b[0;36mextract_eyelid_features\u001b[0;34m(video, is_grayscale, visualize_results, visualization_output_filepath, safe_execution, keypoint_threshold)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_eyelid_features\u001b[39m(video: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    447\u001b[0m                             is_grayscale: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    448\u001b[0m                             visualize_results: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m                            )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# Extract eyelid features with pylids\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     eyelid_features: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpylids_analyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meyelid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# Check to ensure that the video is well formed\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(safe_execution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m): \n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/BLNK_pipeline/blnk_analysis_code/utility/extract_eye_features.py:248\u001b[0m, in \u001b[0;36mpylids_analyze_video\u001b[0;34m(video, target)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing pylids method, video argument must be string path to video\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Perform pylids feature extraction\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m pylids_out: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpylids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meyelid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msave_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Next, convert format from dict[str, list[dict]] to a list of dictionaries\u001b[39;00m\n\u001b[1;32m    255\u001b[0m pylid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:428\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(eye_vid, model_name, batch_sz, eye_id, estimate_pupils, estimate_eyelids, use_ransac, timestamp_file, dest_folder, npz_file, out_file, progress_bar, constraint_eyefit, save_dlc_output, save_vid, save_npz, annot_lw, annot_clr)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_config_file), model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m config.yaml file not found\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m#a wrapper function which runs DLC to estimate keypoints\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m x,y,c \u001b[38;5;241m=\u001b[39m \u001b[43mdlc_estimate_kpts\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dlc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_pupils\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# main outputs are saved as a list of dicts\u001b[39;00m\n\u001b[1;32m    431\u001b[0m dlc_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:289\u001b[0m, in \u001b[0;36mdlc_estimate_kpts\u001b[0;34m(eye_vid, path_config_file, save_dlc_output, dest_folder, batch_sz, estimate_pupils, estimate_eyelids)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdirname:\n\u001b[0;32m--> 289\u001b[0m         \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#change in pose_config.yml to 1\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmpdirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTORCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal config files in \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tmpdirname) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/compat.py:954\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, in_random_order, destfolder, batchsize, cropping, TFGPUinference, dynamic, modelprefix, robust_nframes, allow_growth, use_shelve, auto_track, n_tracks, animal_names, calibrate, identity_only, use_openvino, engine, **torch_kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m             torch_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideotype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainingsetindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingsetindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_as_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_as_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_random_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_random_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_shelve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_shelve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_track\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_track\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43manimal_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manimal_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43midentity_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentity_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcropping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function is not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:545\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, save_as_csv, in_random_order, snapshot_index, detector_snapshot_index, device, destfolder, batch_size, detector_batch_size, dynamic, ctd_conditions, ctd_tracking, top_down_dynamic, modelprefix, use_shelve, robust_nframes, transform, auto_track, n_tracks, animal_names, calibrate, identity_only, overwrite, cropping, save_as_df)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m [time\u001b[38;5;241m.\u001b[39mtime()]\n\u001b[0;32m--> 545\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     runtime\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m    553\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _generate_metadata(\n\u001b[1;32m    554\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mproject_cfg,\n\u001b[1;32m    555\u001b[0m         pytorch_config\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mmodel_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[1;32m    563\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:204\u001b[0m, in \u001b[0;36mvideo_inference\u001b[0;34m(video, pose_runner, detector_runner, cropping, shelf_writer, robust_nframes)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m--> 204\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:142\u001b[0m, in \u001b[0;36mInferenceRunner.inference\u001b[0;34m(self, images, shelf_writer)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(data)\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_full_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_results(shelf_writer)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Process the last batch even if not full\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:203\u001b[0m, in \u001b[0;36mInferenceRunner._process_full_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes prepared inputs in batches of the desired batch size.\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:246\u001b[0m, in \u001b[0;36mInferenceRunner._process_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    242\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m     mk: v[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size] \u001b[38;5;28;01mfor\u001b[39;00m mk, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    244\u001b[0m }\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# remove processed inputs from batch\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:303\u001b[0m, in \u001b[0;36mPoseInferenceRunner.predict\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mcrop(inputs)\n\u001b[1;32m    302\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 303\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/model.py:136\u001b[0m, in \u001b[0;36mPoseModel.get_predictions\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    137\u001b[0m         name: head\u001b[38;5;241m.\u001b[39mpredictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strides[name], outputs[name])\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    139\u001b[0m     }\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[1;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/model.py:137\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 137\u001b[0m         name: \u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    139\u001b[0m     }\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[1;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/predictors/single_predictor.py:93\u001b[0m, in \u001b[0;36mHeatmapPredictor.forward\u001b[0;34m(self, stride, outputs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     locrefs \u001b[38;5;241m=\u001b[39m locrefs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     89\u001b[0m         batch_size, height, width, num_joints, \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     locrefs \u001b[38;5;241m=\u001b[39m locrefs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocref_std\n\u001b[0;32m---> 93\u001b[0m poses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pose_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_scores:\n\u001b[1;32m     96\u001b[0m     poses[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(poses[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/predictors/single_predictor.py:-1\u001b[0m, in \u001b[0;36mHeatmapPredictor.get_pose_prediction\u001b[0;34m(self, heatmap, locref, scale_factors)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(blnk_analysis_pipeline)\n",
    "\n",
    "# Iterate over the videos and output them to the target directory \n",
    "for video_num, video_path in enumerate(videos_to_analyze):\n",
    "    print(f\"Processing video: {video_num+1}/{len(videos_to_analyze)}\", flush=True)\n",
    "    blnk_analysis_pipeline.predict_eye_features(video_path, output_folder_path, crop_box, target_size, temp_dir_path, \n",
    "                                                threshold_value=whiteness_threshold,\n",
    "                                                visualize_results=False\n",
    "                                               )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac455f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
